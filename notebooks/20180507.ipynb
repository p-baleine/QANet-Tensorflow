{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "とりあえず少量のデータ量でロスが低下するのを確認できたので、SQuADデータ全体を用いて学習してみた。\n",
    "\n",
    "未実装項目として、(把握している範囲では)dropoutとl2正則化、及びmulti GPU対応がある。最後のmulti GPUについて、GTX 1080だとmodel encoderのブロック数をpaper通りに8に設定するとOOMで落ちてしまった。今のところGTX 1080で前述のブロック数2つまでは動作することを確認している。(GTX 1080 1枚で何ブロックまで乗るのか確認した後2枚に分散するよう処理を修正する。)\n",
    "\n",
    "今回はCSVLoggerを忘れていたのでスクリーンショットを添付する。\n",
    "\n",
    "以下はtrainに対するlossとacc、青いラインは実装の確認用に用意した少量のデータ(SQuADの1/80)の結果、オレンジのラインはSQuADのデータに対する結果:\n",
    "\n",
    "![2018-05-07%20%281%29.png](./2018-05-07.png)\n",
    "\n",
    "以下はdevに対するlossとacc:\n",
    "\n",
    "![2018-05-07%20%281%29.png](./2018-05-07_2.png)\n",
    "\n",
    "実装の確認に用いた青いラインではtrainとdevに同じデータを用いているので過学習はしていない。\n",
    "\n",
    "SQuADデータに対する結果について、2 epoch目から露骨に過学習を始めている。\n",
    "\n",
    "SQuADの公式評価スクリプトの結果は:\n",
    "\n",
    "```\n",
    "{\"f1\": 28.42425913616532, \"exact_match\": 18.732261116367077}\n",
    "```\n",
    "\n",
    "また1 epochに要する時間は(GTX 1080一枚だと)凡そ30分だった。\n",
    "\n",
    "### TODO\n",
    "\n",
    "* Dropout\n",
    "* l2-norm\n",
    "* GTX 1080 1枚でmemoru encoderが何ブロックまで乗るのか確認する\n",
    "* multi-GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
