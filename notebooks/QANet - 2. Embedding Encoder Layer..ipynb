{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "position encoding + [convolution-layer × # + self-attention-layer + feed-forward-layer]\n",
    "\n",
    "> We use depthwise separable convolutions (Chollet, 2016) (Kaiser et al., 2017) rather than traditional ones, as we observe that it is memory efficient and has better generalization.\n",
    "\n",
    "これはkerasが持ってくれている - [SeparableConv2D](https://keras.io/layers/convolutional/#separableconv2d)\n",
    "\n",
    "> For the self-attention-layer, we adopt the multi-head attention mechanism\n",
    "\n",
    "これは自前実装が必要そう\n",
    "\n",
    "勉強だから自分で実装するけど、答えは[ここ](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_attention.py#L2557)\n",
    "\n",
    "> A positional encoding is added to the input at the beginning of each encoder layer consisting of sin and cos functions at varying wavelengths, as defined in (Vaswani et al., 2017a). \n",
    "\n",
    "これも自前かな？元論文にあたる\n",
    "\n",
    "tensor2tensorに[実装](https://github.com/tensorflow/tensor2tensor/blob/ed9e3bdfd0292d4b6e5b1a1bf272146c8e2f5e9f/tensor2tensor/layers/common_attention.py#L386)がある。これ使わせてもらいたいなぁ\n",
    "\n",
    "tensor2tensor自体をpip installしてimportできちゃった、超横着\n",
    "\n",
    "(これやりだすとmulti-head attentionだってimportできちゃうんじゃないのか？勉強したいから自分で書くつもりだけど…)\n",
    "\n",
    "> Each of these basic operations (conv/self-attention/ffn) is placed inside a residual block\n",
    "\n",
    "これも自前かな？\n",
    "\n",
    "ドロップアウト系は後で見直しが必要だな\n",
    "\n",
    "layernormとかresidual networkとかはtransformerでもやっている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensor2tensor.layers.common_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor2tensor.layers.common_attention.get_timing_signal_1d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled Dot-Product Attention\n",
    "\n",
    "d_k = 20\n",
    "d_v = 24\n",
    "n = 5\n",
    "\n",
    "Q = np.random.randn(n, d_k).astype(np.float32)\n",
    "K = np.random.randn(n, d_k).astype(np.float32)\n",
    "V = np.random.randn(n, d_v).astype(np.float32)\n",
    "\n",
    "def dot_product_attention(Q, K, V):\n",
    "    d_k = tf.cast(K.shape[1], tf.float32)\n",
    "    return tf.matmul(tf.nn.softmax(\n",
    "        tf.matmul(Q, tf.transpose(K)) / tf.square(d_k)), V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=5051, shape=(5, 24), dtype=float32, numpy=\n",
       "array([[-0.06497877, -0.26551306, -0.4132626 , -0.26437664,  0.20844099,\n",
       "        -0.58506936, -0.34095284, -0.5295125 , -0.1359812 , -0.21008614,\n",
       "        -0.7925345 , -0.04303828, -0.2600369 , -0.40712565, -0.31213376,\n",
       "        -0.33738914,  0.0243742 ,  0.05142229, -0.15877959,  0.50202566,\n",
       "        -0.17281958, -0.3236704 ,  0.45624834, -0.448968  ],\n",
       "       [-0.06590389, -0.26416665, -0.4176786 , -0.26434335,  0.20476946,\n",
       "        -0.5923201 , -0.3418126 , -0.5398475 , -0.13728186, -0.21365632,\n",
       "        -0.78682137, -0.02977782, -0.26360416, -0.4075202 , -0.31178504,\n",
       "        -0.3322509 ,  0.02127411,  0.04507902, -0.13703358,  0.51651335,\n",
       "        -0.17079738, -0.32865644,  0.44824174, -0.45635408],\n",
       "       [-0.06585614, -0.2681728 , -0.41098645, -0.2697986 ,  0.2002095 ,\n",
       "        -0.58859086, -0.33531162, -0.5287464 , -0.13438258, -0.21499233,\n",
       "        -0.7892546 , -0.04245362, -0.2618675 , -0.400478  , -0.31430125,\n",
       "        -0.33224797,  0.02077492,  0.04610928, -0.16057572,  0.4982558 ,\n",
       "        -0.17346744, -0.32362184,  0.45405212, -0.45154735],\n",
       "       [-0.06401449, -0.26782942, -0.40146977, -0.27523947,  0.19953917,\n",
       "        -0.586621  , -0.3356724 , -0.5292405 , -0.131619  , -0.21108599,\n",
       "        -0.7889494 , -0.04822135, -0.2564212 , -0.39811414, -0.3121382 ,\n",
       "        -0.33321846,  0.01957393,  0.04709098, -0.16632232,  0.49135634,\n",
       "        -0.17739075, -0.31604904,  0.45410097, -0.44914064],\n",
       "       [-0.06795907, -0.25709087, -0.4080593 , -0.26327565,  0.20734352,\n",
       "        -0.5907131 , -0.33806977, -0.53715384, -0.13203147, -0.21580587,\n",
       "        -0.7859826 , -0.04123685, -0.25325906, -0.411377  , -0.30742761,\n",
       "        -0.3343141 ,  0.02795865,  0.04311668, -0.15081638,  0.4973566 ,\n",
       "        -0.17380321, -0.32075146,  0.4564826 , -0.44605476]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_product_attention(Q, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_modelってなんぞ？embeddingの出力、attentnionの入力と出力\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, d_k, d_v, **kwargs):\n",
    "        self._num_heads = num_heads\n",
    "        self._d_k = d_k\n",
    "        self._d_v = d_v\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        Q_shape, K_shape, V_shape = input_shape\n",
    "        d_model = Q_shape[-1]\n",
    "        \n",
    "        W_Q, W_K, W_V = [], [], []\n",
    "        \n",
    "        # 次元がよくわからん\n",
    "        # tensor2tensorより\n",
    "        # query_ antecedent: a Tensor with shape [batch, length_q, channels]\n",
    "        # channelって…\n",
    "        # dotproductのところでQはd_kって書いてあっったけど、\n",
    "        # d_kだとW_Q(d_model, d_k)と内積とれない…？\n",
    "        # あと、d_modelは入力と書いてあったけど、じゃぁd_kはどっから来たんだ？\n",
    "        \n",
    "        # Instead of performing a single attention function with\n",
    "        # dmodel-dimensional keys, values and queries,\n",
    "        # we found it beneficial to linearly project the queries,\n",
    "        # keys and values h times with different, learned linear\n",
    "        # projections to dk, dk and dv dimensions, respectively. \n",
    "        \n",
    "        # ここらへんか？dot-productをd_model = d_k = d_vの設定で\n",
    "        # 一回だけ計算するよりも、ってこと？\n",
    "        # 「d_k、d_k、d_vへの異なる学習可能な線形写像」\n",
    "        \n",
    "        # length_qとかがわからん\n",
    "        # このattentionも感覚的に分かるようなわからないような…\n",
    "        \n",
    "        for idx in range(self._num_heads):\n",
    "            W_Q.append(tf.get_variable(\n",
    "                'W_Q_{}'.format(idx), [d_model, self._d_k]))\n",
    "            W_K.append(tf.get_variable(\n",
    "                'W_K_{}'.format(idx), [d_model, self._d_k]))\n",
    "            W_V.append(tf.get_variable(\n",
    "                'W_V_{}'.format(idx), [d_model, self._d_v]))\n",
    "\n",
    "        self._W_Q, self._W_K, self._W_V = W_Q, W_K, W_V\n",
    "        self._W_O = tf.get_variable(\n",
    "            'W_O', [self._num_heads * self._d_v, d_model])\n",
    "        \n",
    "        return super(MultiHeadAttention, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        Q, K, V = x\n",
    "        heads = []\n",
    "        \n",
    "        for idx in range(self._num_heads):\n",
    "            a = tf.matmul(Q, self._W_Q[idx])\n",
    "            attention = dot_product_attention(\n",
    "                a,\n",
    "                tf.matmul(K, self._W_K[idx]),\n",
    "                tf.matmul(V, self._W_V[idx]))\n",
    "            heads.append(attention)\n",
    "        \n",
    "        # (batch_size, h * d_v)\n",
    "        heads = tf.concat(heads, 1)\n",
    "        \n",
    "        # (batch_size, d_model)\n",
    "        return tf.matmul(heads, self._W_O)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k = 20\n",
    "d_v = 24\n",
    "d_model = 12\n",
    "n = 5\n",
    "\n",
    "inputs = [tf.keras.layers.Input(shape=(d_model,)) for _ in range(3)]\n",
    "multi_head_attention = MultiHeadAttention(8, d_k, d_v)(inputs)\n",
    "\n",
    "model = tf.keras.models.Model(\n",
    "    inputs=inputs, outputs=multi_head_attention)\n",
    "model.compile(\n",
    "    optimizer=tf.train.GradientDescentOptimizer(0.001),\n",
    "    loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Q, K, V = [np.random.randn(n, d_model).astype(np.float32) for _ in range(3)]\n",
    "\n",
    "a = model.predict([Q, K, V])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 12)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
