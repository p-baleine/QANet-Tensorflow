{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word embedding\n",
    "\n",
    "batch_size = 2\n",
    "N = 10 # number of maximum context length\n",
    "M = 5 # number of maximum question length\n",
    "p1 = 300 # word embedding size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> All the out-of-vocabulary words are mapped to an <UNK> token, whose embedding is trainable with random initialization. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "class Inputs(namedtuple('Inputs', [\n",
    "    'context_words',\n",
    "    'context_word_unk_label'])):\n",
    "    pass\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "inputs = Inputs(\n",
    "    context_words=np.random.randn(batch_size, N, p1),\n",
    "    # 最初だけUNK\n",
    "    context_word_unk_label=np.array([[True] + [False] * (N - 1)] * batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_word_embedding = tf.get_variable('unk_word_embedding', shape=(1, p1))\n",
    "\n",
    "context_words = tf.where(\n",
    "    tf.tile(tf.expand_dims(inputs.context_word_unk_label, -1), [1, 1, p1]),\n",
    "    tf.nn.embedding_lookup(\n",
    "        unk_word_embedding,\n",
    "        tf.zeros_like(inputs.context_word_unk_label, dtype=tf.int32)),\n",
    "    inputs.context_words)\n",
    "\n",
    "assert not all(np.isclose(\n",
    "    context_words[0][0],\n",
    "    inputs.context_words[0][0]))\n",
    "assert all(np.isclose(\n",
    "    context_words[0][1],\n",
    "    inputs.context_words[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The character embedding is obtained as follows: Each character is represented as a trainable vector of dimension p2 = 200, meaning each word can be viewed as the concatenation of the embedding vectors for each of its characters. The length of each word is either truncated or padded to 16. We take maximum value of each row of this matrix to get a fixed-size vector representation of each word. Finally, the output of a given word x from this layer is the concatenation [xw;xc] ∈ Rp1+p2, where xw and xc are the word embedding and the convolution output of character embedding of x respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 16 # number of maximum word length\n",
    "char_vocab_size = 1000\n",
    "p2 = 200 # character embedding size\n",
    "\n",
    "class Inputs(namedtuple('Inputs', [\n",
    "    'context_words',\n",
    "    'context_word_unk_label',\n",
    "    'context_char_ids'])):\n",
    "    pass\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "inputs = Inputs(\n",
    "    context_words=np.random.randn(batch_size, N, p1),\n",
    "    # 最初だけUNK\n",
    "    context_word_unk_label=np.array([[True] + [False] * (N - 1)] * batch_size),\n",
    "    context_char_ids=np.random.randint(0, char_vocab_size, size=(batch_size, N, C)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_char_embedding = tf.get_variable(\n",
    "    'context_char_embedding', shape=(char_vocab_size, p2))\n",
    "\n",
    "# (batch_size, N, C, p2)\n",
    "context_chars = tf.nn.embedding_lookup(\n",
    "    context_char_embedding, inputs.context_char_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=82, shape=(20, 1, 200), dtype=float32, numpy=\n",
       "array([[[0.08703647, 0.10834691, 0.05828177, ..., 0.04130189,\n",
       "         0.06452552, 0.06336806]],\n",
       "\n",
       "       [[0.04885535, 0.07087737, 0.09226833, ..., 0.03336651,\n",
       "         0.06198725, 0.08716781]],\n",
       "\n",
       "       [[0.05197889, 0.08440043, 0.05774486, ..., 0.08644083,\n",
       "         0.06589647, 0.0719725 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.09894997, 0.07307689, 0.05758217, ..., 0.03159501,\n",
       "         0.05708284, 0.09963546]],\n",
       "\n",
       "       [[0.06574344, 0.03455252, 0.04887037, ..., 0.03554013,\n",
       "         0.07093097, 0.098703  ]],\n",
       "\n",
       "       [[0.06985399, 0.08604864, 0.04421144, ..., 0.08145757,\n",
       "         0.09867382, 0.13105427]]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conv2d: (batch_size, height, width, in_channel)\n",
    "#    -> (batch_size, height - f + 1, width - f + 1, out_channel)\n",
    "\n",
    "# 今回の場合: (batch_size, 1, C, p2)\n",
    "#    -> (batch_size, 1, C * f + 1, p2)\n",
    "# 但しbatch_sizeは実はbatch_size * N\n",
    "\n",
    "filter_size = 7\n",
    "\n",
    "# (batch_size * N, 1, C, p2)\n",
    "context_chars = tf.expand_dims(tf.reshape(context_chars, [-1, C, p2]), 1)\n",
    "kernel = tf.get_variable('char_filter', [1, filter_size, p2, p2])\n",
    "# (batch_size * N, 1, C - filter_size + 1, p2)\n",
    "context_chars = tf.nn.conv2d(\n",
    "    context_chars, kernel, [1, 1, 1, 1], 'VALID')\n",
    "# (batch_size, N, C - filter_size + 1, p2)\n",
    "#context_chars = tf.reshape(context_chars, [-1, N, C - filter_size + 1, p2])\n",
    "# (batch_size, N, p2)\n",
    "context_chars = tf.reduce_max(context_chars, 2)\n",
    "context_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class WordEmbedding(tf.keras.layers.Layer):\n",
    "    def build(self, input_shape):\n",
    "        word_input_shape, _ = input_shape\n",
    "        word_embedding_dim = word_input_shape[-1]\n",
    "        \n",
    "        self._unk_embedding = self.add_weight(\n",
    "            'unk_embedding',\n",
    "            [1, word_embedding_dim],\n",
    "            initializer='glorot_uniform')\n",
    "\n",
    "        super(WordEmbedding, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        \"\"\"UNKラベルで指定された単語をUNK向けのembeddingで\n",
    "        置き換える\n",
    "        \n",
    "          x:\n",
    "            words: (batch_size, N, word_embedding_dim)\n",
    "            word_unk_label: (batch_size, N)\n",
    "        \"\"\"\n",
    "        words, word_unk_label = x\n",
    "        # なぜかfloatで渡ってくる…\n",
    "        word_unk_label = tf.cast(word_unk_label, tf.bool)\n",
    "        \n",
    "        # All the out-of-vocabulary words are mapped to an <UNK> token,\n",
    "        # whose embedding is trainable with random initialization. \n",
    "        \n",
    "        # (batch_size, N, p1)\n",
    "        return tf.where(\n",
    "            tf.tile(tf.expand_dims(word_unk_label, -1), [1, 1, words.shape[-1]]),\n",
    "            tf.nn.embedding_lookup(\n",
    "                self._unk_embedding,\n",
    "                tf.zeros_like(word_unk_label, dtype=tf.int32)),\n",
    "            words)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0]\n",
    "\n",
    "class CharacterEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, dim, filter_size, **kwargs):\n",
    "        \"\"\"コンストラクタ\n",
    "        \n",
    "          vocab_size: 文字の辞書のサイズ\n",
    "          dim: embeddingの次元\n",
    "          filter_size: 畳み込みのフィルターサイズ\n",
    "        \"\"\"\n",
    "        self._vocab_size = vocab_size\n",
    "        self._dim = dim\n",
    "        self._filter_size = filter_size\n",
    "        super(CharacterEmbedding, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self._embedding = self.add_weight(\n",
    "            'embedding',\n",
    "            [self._vocab_size, self._dim],\n",
    "            initializer='glorot_uniform')\n",
    "        self._filter = self.add_weight(\n",
    "            'filter',\n",
    "            [1, self._filter_size, self._dim, self._dim],\n",
    "            initializer='glorot_uniform')\n",
    "        self._bias = self.add_weight(\n",
    "            'bias',\n",
    "            [1, 1, 1, self._dim],\n",
    "            initializer='zeros')\n",
    "        super(CharacterEmbedding, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        \"\"\"文字のIDをdimのベクトルに変換する\n",
    "        \n",
    "          x: (batch_size, N, C)\n",
    "              Cは最大文字数(16)\n",
    "        \"\"\"\n",
    "        x = tf.cast(x, tf.int32)\n",
    "        N, C = x.shape[1:]\n",
    "\n",
    "        # from BiDAF\n",
    "        # Characters are embed- ded into vectors, which can be\n",
    "        # considered as 1D inputs to the CNN, and whose size is\n",
    "        # the input channel size of the CNN.\n",
    "        # The outputs of the CNN are max-pooled over the entire\n",
    "        # width to obtain a fixed-size vector for each word.\n",
    "        \n",
    "        # (batch_size, N, C, p2)\n",
    "        x_ = tf.nn.embedding_lookup(self._embedding, x)\n",
    "        # (batch_size * N, C, p2)\n",
    "        x_ = tf.reshape(x_, [-1, C, self._dim])\n",
    "        # (batch_size * N, 1, C, p2)\n",
    "        x_ = tf.expand_dims(x_, 1)\n",
    "        # (batch_size * N, 1, C - filter_size + 1, p2)\n",
    "        x_ = tf.nn.conv2d(x_, self._filter, [1, 1, 1, 1], 'VALID') + self._bias\n",
    "        # (batch_size, N, C - filter_size + 1, p2)\n",
    "        x_ = tf.reshape(x_, [-1, N, C - self._filter_size + 1, self._dim])\n",
    "        # (batch_size, N, p2)\n",
    "        return tf.reduce_max(tf.nn.relu(x_), 2)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tf.TensorShape(\n",
    "            [input_shape[0], input_shape[1], self._dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeferredTensor('None', shape=(?, 10, 200), dtype=float32)\n",
      "DeferredTensor('None', shape=(?, 10, 300), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "N = 10 # number of maximum context length\n",
    "M = 5 # number of maximum question length\n",
    "p1 = 300 # word embedding size\n",
    "C = 16 # number of maximum word length\n",
    "char_vocab_size = 1000\n",
    "p2 = 200 # character embedding size\n",
    "filter_size = 7\n",
    "\n",
    "context_words = tf.keras.layers.Input(shape=(N, p1))\n",
    "context_word_unk_label = tf.keras.layers.Input(shape=(N,))\n",
    "context_char_ids = tf.keras.layers.Input(shape=(N, C), dtype='int32')\n",
    "\n",
    "word_embedding_layer = WordEmbedding()\n",
    "char_embedding_layer = CharacterEmbedding(\n",
    "    char_vocab_size, p2, filter_size)\n",
    "\n",
    "context_word_emb = word_embedding_layer(\n",
    "    [context_words, context_word_unk_label])\n",
    "context_char_emb = char_embedding_layer(context_char_ids)\n",
    "\n",
    "print(context_char_emb)\n",
    "print(context_word_emb)\n",
    "\n",
    "con = tf.keras.layers.Concatenate(axis=2)\n",
    "context = con(\n",
    "    [context_word_emb, context_char_emb])\n",
    "\n",
    "model = tf.keras.models.Model(\n",
    "    inputs=[\n",
    "        context_words,\n",
    "        context_word_unk_label,\n",
    "        context_char_ids],\n",
    "    outputs=context)\n",
    "model.compile(\n",
    "    optimizer=tf.train.GradientDescentOptimizer(0.001),\n",
    "    loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "class Inputs_(namedtuple('Inputs_', [\n",
    "    'context_words',\n",
    "    'context_word_unk_label',\n",
    "    'context_char_ids'])):\n",
    "    pass\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "inputs = Inputs_(\n",
    "    context_words=np.random.randn(batch_size, N, p1),\n",
    "    # 最初だけUNK\n",
    "    context_word_unk_label=np.array([[True] + [False] * (N - 1)] * batch_size),\n",
    "    context_char_ids=np.random.randint(0, char_vocab_size, size=(batch_size, N, C)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 10, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 10, 16)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding_3 (WordEmbedding (None, 10, 300)      300         input_7[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "character_embedding_3 (Characte (None, 10, 200)      480200      input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 10, 500)      0           word_embedding_3[0][0]           \n",
      "                                                                 character_embedding_3[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 480,500\n",
      "Trainable params: 480,500\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(list(inputs)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The concatenation of the character and word embedding vectors is passed to a two-layer Highway Network (Srivastava et al., 2015).\n",
    "\n",
    "BiDAFより"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighwayLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(HighwayLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        d = input_shape[-1]\n",
    "        \n",
    "        self._W_T = self.add_weight(\n",
    "            'weight_transform',\n",
    "            [d, d],\n",
    "            initializer='glorot_uniform')\n",
    "        self._b_T = self.add_weight(\n",
    "            'bias_transform',\n",
    "            [d],\n",
    "            initializer='zeros')\n",
    "        self._W = self.add_weight(\n",
    "            'weight',\n",
    "            [d, d],\n",
    "            initializer='glorot_uniform')\n",
    "        self._b = self.add_weight(\n",
    "            'bias',\n",
    "            [d],\n",
    "            initializer='zeros')\n",
    "    \n",
    "    def call(self, input):\n",
    "        T = tf.sigmoid(tf.matmul(input, self._W_T) + self._b_T)\n",
    "        H = tf.nn.relu(tf.matmul(input, self._W) + self._b)\n",
    "        return H * T + (1. - T) * input\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(10,))\n",
    "\n",
    "x = HighwayLayer()(inputs)\n",
    "\n",
    "model = tf.keras.models.Model(\n",
    "    inputs=inputs, outputs=x)\n",
    "model.compile(\n",
    "    optimizer=tf.train.GradientDescentOptimizer(0.001),\n",
    "    loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighwayNetwork(tf.keras.models.Model):\n",
    "    def __init__(self, num_layers, **kwargs):\n",
    "        self._layers = [HighwayLayer() for _ in range(num_layers)]\n",
    "        super(HighwayNetwork, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, input):\n",
    "        y = input\n",
    "        for layer in self._layers:\n",
    "            y = layer(y)\n",
    "        return y\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(10, 500))\n",
    "\n",
    "x = HighwayNetwork(num_layers=2)(inputs)\n",
    "\n",
    "model = tf.keras.models.Model(\n",
    "    inputs=inputs, outputs=x)\n",
    "model.compile(\n",
    "    optimizer=tf.train.GradientDescentOptimizer(0.001),\n",
    "    loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 10, 500)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.random.randn(2, 10, 500)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "やっとInput Embedding Layer終わり"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
